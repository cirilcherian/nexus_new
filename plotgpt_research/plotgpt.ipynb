{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting langchain_experimental\n",
      "  Obtaining dependency information for langchain_experimental from https://files.pythonhosted.org/packages/3d/40/37424a8353557cbfbe9757ccfd859532fa90e8b010e583d04f283785f56d/langchain_experimental-0.0.55-py3-none-any.whl.metadata\n",
      "  Downloading langchain_experimental-0.0.55-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langchain<0.2.0,>=0.1.13 (from langchain_experimental)\n",
      "  Obtaining dependency information for langchain<0.2.0,>=0.1.13 from https://files.pythonhosted.org/packages/f8/1b/697dec4ff03114b049b687d4fdbdcefdfff365868876ec58c1ab2cf75253/langchain-0.1.13-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.1.13-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.33 (from langchain_experimental)\n",
      "  Obtaining dependency information for langchain-core<0.2.0,>=0.1.33 from https://files.pythonhosted.org/packages/d9/09/2aca6c80eed501c2de0990bb7b8b353f4baf67ad1f4117f1539e67847868/langchain_core-0.1.33-py3-none-any.whl.metadata\n",
      "  Downloading langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain<0.2.0,>=0.1.13->langchain_experimental) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain<0.2.0,>=0.1.13->langchain_experimental) (2.0.27)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain<0.2.0,>=0.1.13->langchain_experimental) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain<0.2.0,>=0.1.13->langchain_experimental) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain<0.2.0,>=0.1.13->langchain_experimental) (1.33)\n",
      "Collecting langchain-community<0.1,>=0.0.29 (from langchain<0.2.0,>=0.1.13->langchain_experimental)\n",
      "  Obtaining dependency information for langchain-community<0.1,>=0.0.29 from https://files.pythonhosted.org/packages/e1/80/4136757bf245f60e2f4e148adb340207ea330be92bc26727bca32fe6bc48/langchain_community-0.0.29-py3-none-any.whl.metadata\n",
      "  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain<0.2.0,>=0.1.13->langchain_experimental) (0.0.1)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain<0.2.0,>=0.1.13->langchain_experimental)\n",
      "  Obtaining dependency information for langsmith<0.2.0,>=0.1.17 from https://files.pythonhosted.org/packages/f7/1d/6b238672b68c46db68b3682d181b68176feebd573cd084f87eea01fc54f9/langsmith-0.1.31-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.1.31-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain<0.2.0,>=0.1.13->langchain_experimental) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain<0.2.0,>=0.1.13->langchain_experimental) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain<0.2.0,>=0.1.13->langchain_experimental) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain<0.2.0,>=0.1.13->langchain_experimental) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain_experimental) (3.5.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain_experimental) (23.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.13->langchain_experimental) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.13->langchain_experimental) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.13->langchain_experimental) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.13->langchain_experimental) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.13->langchain_experimental) (1.8.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain_experimental) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain_experimental) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.13->langchain_experimental) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.13->langchain_experimental) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.13->langchain_experimental) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.13->langchain_experimental) (3.9.15)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.13->langchain_experimental) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.13->langchain_experimental) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.13->langchain_experimental) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.13->langchain_experimental) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.13->langchain_experimental) (2.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.13->langchain_experimental) (1.0.0)\n",
      "Downloading langchain_experimental-0.0.55-py3-none-any.whl (177 kB)\n",
      "   ---------------------------------------- 0.0/177.6 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 112.6/177.6 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 177.6/177.6 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading langchain-0.1.13-py3-none-any.whl (810 kB)\n",
      "   ---------------------------------------- 0.0/810.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 810.5/810.5 kB 25.8 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.1.33-py3-none-any.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 269.1/269.1 kB 16.2 MB/s eta 0:00:00\n",
      "Downloading langchain_community-0.0.29-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 1.5/1.8 MB 47.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 38.0 MB/s eta 0:00:00\n",
      "Downloading langsmith-0.1.31-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.6/71.6 kB ? eta 0:00:00\n",
      "Installing collected packages: langsmith, langchain-core, langchain-community, langchain, langchain_experimental\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.9\n",
      "    Uninstalling langsmith-0.1.9:\n",
      "      Successfully uninstalled langsmith-0.1.9\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.30\n",
      "    Uninstalling langchain-core-0.1.30:\n",
      "      Successfully uninstalled langchain-core-0.1.30\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.0.24\n",
      "    Uninstalling langchain-community-0.0.24:\n",
      "      Successfully uninstalled langchain-community-0.0.24\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.1.9\n",
      "    Uninstalling langchain-0.1.9:\n",
      "      Successfully uninstalled langchain-0.1.9\n",
      "Successfully installed langchain-0.1.13 langchain-community-0.0.29 langchain-core-0.1.33 langchain_experimental-0.0.55 langsmith-0.1.31\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_experimental\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"biostats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>\"Sex\"</th>\n",
       "      <th>\"Age\"</th>\n",
       "      <th>\"Height (in)\"</th>\n",
       "      <th>\"Weight (lbs)\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alex</td>\n",
       "      <td>\"M\"</td>\n",
       "      <td>41</td>\n",
       "      <td>74</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bert</td>\n",
       "      <td>\"M\"</td>\n",
       "      <td>42</td>\n",
       "      <td>68</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carl</td>\n",
       "      <td>\"M\"</td>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dave</td>\n",
       "      <td>\"M\"</td>\n",
       "      <td>39</td>\n",
       "      <td>72</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elly</td>\n",
       "      <td>\"F\"</td>\n",
       "      <td>30</td>\n",
       "      <td>66</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name       \"Sex\"   \"Age\"   \"Height (in)\"   \"Weight (lbs)\"\n",
       "0  Alex         \"M\"      41              74              170\n",
       "1  Bert         \"M\"      42              68              166\n",
       "2  Carl         \"M\"      32              70              155\n",
       "3  Dave         \"M\"      39              72              167\n",
       "4  Elly         \"F\"      30              66              124"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_pandas_dataframe_agent(openai, data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"If you plot anything then save the plot to plot.png.consider both upper and lower case variations \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is the average weight\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use the mean function from pandas to calculate the average weight\n",
      "Action: python_repl_ast\n",
      "Action Input: df[\"Weight (lbs)\"].mean()\u001b[0m\u001b[36;1m\u001b[1;3mKeyError: 'Weight (lbs)'\u001b[0m\u001b[32;1m\u001b[1;3mI need to make sure the column name is correct\n",
      "Action: python_repl_ast\n",
      "Action Input: df[\"Weight (lbs)\"].mean()\u001b[0m\u001b[36;1m\u001b[1;3mKeyError: 'Weight (lbs)'\u001b[0m\u001b[32;1m\u001b[1;3mI need to check the column names in the dataframe\n",
      "Action: python_repl_ast\n",
      "Action Input: df.columns\u001b[0m\u001b[36;1m\u001b[1;3mIndex(['Name', '     \"Sex\"', ' \"Age\"', ' \"Height (in)\"', ' \"Weight (lbs)\"'], dtype='object')\u001b[0m\u001b[32;1m\u001b[1;3m I need to remove the extra spaces in the column names\n",
      "Action: python_repl_ast\n",
      "Action Input: df.columns = df.columns.str.strip()\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m Now I can calculate the average weight\n",
      "Action: python_repl_ast\n",
      "Action Input: df[\"Weight (lbs)\"].mean()\u001b[0m\u001b[36;1m\u001b[1;3mKeyError: 'Weight (lbs)'\u001b[0m\u001b[32;1m\u001b[1;3m I need to remove the quotes from the column names\n",
      "Action: python_repl_ast\n",
      "Action Input: df.columns = df.columns.str.replace('\"', '')\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m Now I can calculate the average weight\n",
      "Action: python_repl_ast\n",
      "Action Input: df[\"Weight (lbs)\"].mean()\u001b[0m\u001b[36;1m\u001b[1;3m146.72222222222223\u001b[0m\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 146.72222222222223\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent(question + prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'what is the average weightIf you plot anything then save the plot to plot.png.consider both upper and lower case variations ', 'output': '146.72222222222223'}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################################CHAT PLOT WITH YOUR CSV##############################################            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The dataframe appears to contain information about individuals, including their name, sex, age, height, and weight. \n",
      "\n",
      "Final Answer: The dataframe contains five columns: \"Name\", \"Sex\", \"Age\", \"Height (in)\", and \"Weight (lbs)\". The \"Name\" column contains the names of the individuals. The \"Sex\" column indicates the sex of the individuals, with \"M\" for male and \"F\" for female. The \"Age\" column contains the ages of the individuals. The \"Height (in)\" column contains the heights of the individuals in inches. The \"Weight (lbs)\" column contains the weights of the individuals in pounds.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'explain data frame generated.',\n",
       " 'output': 'The dataframe contains five columns: \"Name\", \"Sex\", \"Age\", \"Height (in)\", and \"Weight (lbs)\". The \"Name\" column contains the names of the individuals. The \"Sex\" column indicates the sex of the individuals, with \"M\" for male and \"F\" for female. The \"Age\" column contains the ages of the individuals. The \"Height (in)\" column contains the heights of the individuals in inches. The \"Weight (lbs)\" column contains the weights of the individuals in pounds.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "import matplotlib.image as mpimg\n",
    "import base64\n",
    "from langchain_openai import OpenAI,ChatOpenAI\n",
    "# openai = OpenAI(temperature=0.5)\n",
    "# data = pd.read_csv(\"biostats.csv\")\n",
    "data = pd.read_csv(\"biostats.csv\")\n",
    "# agent = create_pandas_dataframe_agent(openai, data, verbose=True)\n",
    "# question = \"what is the average weight\"\n",
    "# response = agent(question + prompt)\n",
    "agent = create_pandas_dataframe_agent(ChatOpenAI(temperature=0,model=\"gpt-4\"), data, verbose=True)\n",
    "question = \"explain data frame generated.\"\n",
    "# question = \"explain this in a big paragraph.mention all the column names .analyse the data and give a description about this\"\n",
    "response = agent(question)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       Row ID        Order ID  Order Date   Ship Date       Ship Mode  \\\n",
       "0          1  CA-2016-152156   11/8/2016  11/11/2016    Second Class   \n",
       "1          2  CA-2016-152156   11/8/2016  11/11/2016    Second Class   \n",
       "2          3  CA-2016-138688   6/12/2016   6/16/2016    Second Class   \n",
       "3          4  US-2015-108966  10/11/2015  10/18/2015  Standard Class   \n",
       "4          5  US-2015-108966  10/11/2015  10/18/2015  Standard Class   \n",
       "...      ...             ...         ...         ...             ...   \n",
       "9989    9990  CA-2014-110422   1/21/2014   1/23/2014    Second Class   \n",
       "9990    9991  CA-2017-121258   2/26/2017    3/3/2017  Standard Class   \n",
       "9991    9992  CA-2017-121258   2/26/2017    3/3/2017  Standard Class   \n",
       "9992    9993  CA-2017-121258   2/26/2017    3/3/2017  Standard Class   \n",
       "9993    9994  CA-2017-119914    5/4/2017    5/9/2017    Second Class   \n",
       "\n",
       "     Customer ID     Customer Name    Segment        Country             City  \\\n",
       "0       CG-12520       Claire Gute   Consumer  United States        Henderson   \n",
       "1       CG-12520       Claire Gute   Consumer  United States        Henderson   \n",
       "2       DV-13045   Darrin Van Huff  Corporate  United States      Los Angeles   \n",
       "3       SO-20335    Sean O'Donnell   Consumer  United States  Fort Lauderdale   \n",
       "4       SO-20335    Sean O'Donnell   Consumer  United States  Fort Lauderdale   \n",
       "...          ...               ...        ...            ...              ...   \n",
       "9989    TB-21400  Tom Boeckenhauer   Consumer  United States            Miami   \n",
       "9990    DB-13060       Dave Brooks   Consumer  United States       Costa Mesa   \n",
       "9991    DB-13060       Dave Brooks   Consumer  United States       Costa Mesa   \n",
       "9992    DB-13060       Dave Brooks   Consumer  United States       Costa Mesa   \n",
       "9993    CC-12220      Chris Cortes   Consumer  United States      Westminster   \n",
       "\n",
       "           State  Postal Code Region       Product ID         Category  \\\n",
       "0       Kentucky        42420  South  FUR-BO-10001798        Furniture   \n",
       "1       Kentucky        42420  South  FUR-CH-10000454        Furniture   \n",
       "2     California        90036   West  OFF-LA-10000240  Office Supplies   \n",
       "3        Florida        33311  South  FUR-TA-10000577        Furniture   \n",
       "4        Florida        33311  South  OFF-ST-10000760  Office Supplies   \n",
       "...          ...          ...    ...              ...              ...   \n",
       "9989     Florida        33180  South  FUR-FU-10001889        Furniture   \n",
       "9990  California        92627   West  FUR-FU-10000747        Furniture   \n",
       "9991  California        92627   West  TEC-PH-10003645       Technology   \n",
       "9992  California        92627   West  OFF-PA-10004041  Office Supplies   \n",
       "9993  California        92683   West  OFF-AP-10002684  Office Supplies   \n",
       "\n",
       "     Sub-Category                                       Product Name  \\\n",
       "0       Bookcases                  Bush Somerset Collection Bookcase   \n",
       "1          Chairs  Hon Deluxe Fabric Upholstered Stacking Chairs,...   \n",
       "2          Labels  Self-Adhesive Address Labels for Typewriters b...   \n",
       "3          Tables      Bretford CR4500 Series Slim Rectangular Table   \n",
       "4         Storage                     Eldon Fold 'N Roll Cart System   \n",
       "...           ...                                                ...   \n",
       "9989  Furnishings                             Ultra Door Pull Handle   \n",
       "9990  Furnishings  Tenex B1-RE Series Chair Mats for Low Pile Car...   \n",
       "9991       Phones                              Aastra 57i VoIP phone   \n",
       "9992        Paper  It's Hot Message Books with Stickers, 2 3/4\" x 5\"   \n",
       "9993   Appliances  Acco 7-Outlet Masterpiece Power Center, Wihtou...   \n",
       "\n",
       "         Sales  Quantity  Discount    Profit  \n",
       "0     261.9600         2      0.00   41.9136  \n",
       "1     731.9400         3      0.00  219.5820  \n",
       "2      14.6200         2      0.00    6.8714  \n",
       "3     957.5775         5      0.45 -383.0310  \n",
       "4      22.3680         2      0.20    2.5164  \n",
       "...        ...       ...       ...       ...  \n",
       "9989   25.2480         3      0.20    4.1028  \n",
       "9990   91.9600         2      0.00   15.6332  \n",
       "9991  258.5760         2      0.20   19.3932  \n",
       "9992   29.6000         4      0.00   13.3200  \n",
       "9993  243.1600         2      0.00   72.9480  \n",
       "\n",
       "[9994 rows x 21 columns]>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The question seems to be asking for an identity, but as an AI model, I don't have a personal identity. I am a language model developed by OpenAI.\n",
      "Final Answer: I am an AI language model developed by OpenAI.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'who are you',\n",
       " 'output': 'I am an AI language model developed by OpenAI.'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "question = \"who are you\"\n",
    "response = agent(question)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################  Mongo DB,CSV loaders ####################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(\n",
    "    file_path=\"biostats.csv\"\n",
    ")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', '\"Sex\"', '\"Age\"', '\"Height (in)\"', '\"Weight (lbs)\"']\n"
     ]
    }
   ],
   "source": [
    "########## Getting Column names####################\n",
    "\n",
    "first_doc = data[0]\n",
    "column_names = []\n",
    "for line in first_doc.page_content.split('\\n'):\n",
    "    key_value = line.split(': ')\n",
    "    if len(key_value) == 2:\n",
    "        column_names.append(key_value[0])\n",
    "\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('converted_biostats.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = column_names\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for doc in data:\n",
    "        row = {}\n",
    "        for line in doc.page_content.split('\\n'):\n",
    "            key_value = line.split(': ')\n",
    "            if len(key_value) == 2:\n",
    "                row[key_value[0]] = key_value[1].strip('\"')\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverted_biostats.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "data.to_csv(\"converted_biostats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Document' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m column_names \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create DataFrame using the column names and remaining data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data[\u001b[38;5;241m1\u001b[39m:], columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:781\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m treat_as_nested(data):\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;66;03m# exclude ndarray as we may have cast it a few lines above\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 781\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    782\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    788\u001b[0m         dtype,\n\u001b[0;32m    789\u001b[0m     )\n\u001b[0;32m    790\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    791\u001b[0m         arrays,\n\u001b[0;32m    792\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    795\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    796\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7128\u001b[0m, in \u001b[0;36mensure_index\u001b[1;34m(index_like, copy)\u001b[0m\n\u001b[0;32m   7126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy, tupleize_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   7127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 7128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:535\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[1;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m data):\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;66;03m# we must be all tuples, otherwise don't construct\u001b[39;00m\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;66;03m# 10697\u001b[39;00m\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiIndex\n\u001b[1;32m--> 535\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m MultiIndex\u001b[38;5;241m.\u001b[39mfrom_tuples(data, names\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    536\u001b[0m \u001b[38;5;66;03m# other iterable of some kind\u001b[39;00m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;66;03m# we allow set/frozenset, which Series/sanitize_array does not, so\u001b[39;00m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;66;03m#  cast to list here\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:211\u001b[0m, in \u001b[0;36mnames_compat.<locals>.new_meth\u001b[1;34m(self_or_cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    209\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m meth(self_or_cls, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:568\u001b[0m, in \u001b[0;36mMultiIndex.from_tuples\u001b[1;34m(cls, tuples, sortorder, names)\u001b[0m\n\u001b[0;32m    565\u001b[0m tuples \u001b[38;5;241m=\u001b[39m cast(Collection[Tuple[Hashable, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]], tuples)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# handling the empty tuple cases\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tuples) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m tuples):\n\u001b[0;32m    569\u001b[0m     codes \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(tuples))]\n\u001b[0;32m    570\u001b[0m     levels \u001b[38;5;241m=\u001b[39m [Index(com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(tuples, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)))]\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'Document' has no len()"
     ]
    }
   ],
   "source": [
    "column_names = data[0]\n",
    "\n",
    "# Create DataFrame using the column names and remaining data\n",
    "df = pd.DataFrame(data[1:], columns=column_names)\n",
    "\n",
    "# Optionally save the DataFrame as a CSV file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_mongodb.vectorstores.MongoDBAtlasVectorSearch at 0x16ed4e3fd10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################## Complete program with mongo ###############\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "# from common_utils.Config import Config\n",
    "# from common_utils.embeddings import default_embeddings\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "client = MongoClient('mongodb+srv://nexus-360-dev-user:tgaiqVncYpj6vOpz@cluster0.ks2kbfx.mongodb.net/')\n",
    "\n",
    "db_name = \"vector_db6\"\n",
    "collection_name = \"vector_collections6\"\n",
    "collection = client[db_name][collection_name]\n",
    "\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=\"biostats.csv\"\n",
    ")\n",
    "data = loader.load()\n",
    "\n",
    "MongoDBAtlasVectorSearch.from_documents(documents = data,embedding=embedding,collection = collection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_uri = 'mongodb+srv://nexus-360-dev-user:tgaiqVncYpj6vOpz@cluster0.ks2kbfx.mongodb.net/'\n",
    "DB_NAME = \"vector_db6\"\n",
    "COLLECTION_NAME = collection_name = \"vector_collections6\"\n",
    "vector_search = MongoDBAtlasVectorSearch.from_connection_string(mongo_uri,DB_NAME + \".\" + COLLECTION_NAME,embedding,index_name=\"vector_index2\")\n",
    "retriever = vector_search.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\n",
    "        \"k\": 1000,\n",
    "        \"score_threshold\": 0.85,\n",
    "        \"pre_filter\" : {\"source\":\"biostats.csv\"}\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-16k\", #\"gpt-3.5-turbo\",\n",
    "    temperature=\"0.3\"\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "combine_docs_chain = create_stuff_documents_chain(model,retrieval_qa_chat_prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever,combine_docs_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  prompt = \"\"\"You are an expert in answering questions from companies' documents. Give answers only based on the context. Don't speculate anything. If the context is not available, and assuming the question isn't a greeting, give the answer as \"You are not allowed to access this data or the data is not in the database.\" If the question is a greeting (example for greeting: hai, hi, hello, can you help me, etc.), you should respond appropriately to the greeting before addressing the issue. When giving the answers, make sure they are in a proper format and add a 'next line' feature in relevant areas in the answer.\"\"\"\n",
    "output = retrieval_chain.invoke({\"input\":\"explain this\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Name: Myra\\n\"Sex\": \"F\"\\n\"Age\": 23\\n\"Height (in)\": 62\\n\"Weight (lbs)\": 98', metadata={'_id': ObjectId('662f3e465e0f0716e50b1b4b'), 'source': 'biostats.csv', 'row': 12})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('1.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = column_names\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for doc in output['context']:\n",
    "        row = {}\n",
    "        for line in doc.page_content.split('\\n'):\n",
    "            key_value = line.split(': ')\n",
    "            if len(key_value) == 2:\n",
    "                row[key_value[0]] = key_value[1].strip('\"')\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', '\"Sex\"', '\"Age\"', '\"Height (in)\"', '\"Weight (lbs)\"']\n"
     ]
    }
   ],
   "source": [
    "first_doc = output['context'][0]\n",
    "column_names = []\n",
    "for line in first_doc.page_content.split('\\n'):\n",
    "    key_value = line.split(': ')\n",
    "    if len(key_value) == 2:\n",
    "        column_names.append(key_value[0])\n",
    "\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>\"Sex\"</th>\n",
       "      <th>\"Age\"</th>\n",
       "      <th>\"Height (in)\"</th>\n",
       "      <th>\"Weight (lbs)\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Myra</td>\n",
       "      <td>F</td>\n",
       "      <td>23</td>\n",
       "      <td>62</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jake</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>69</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carl</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alex</td>\n",
       "      <td>M</td>\n",
       "      <td>41</td>\n",
       "      <td>74</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kate</td>\n",
       "      <td>F</td>\n",
       "      <td>47</td>\n",
       "      <td>69</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name \"Sex\"  \"Age\"  \"Height (in)\"  \"Weight (lbs)\"\n",
       "0  Myra     F     23             62              98\n",
       "1  Jake     M     32             69             143\n",
       "2  Carl     M     32             70             155\n",
       "3  Alex     M     41             74             170\n",
       "4  Kate     F     47             69             139"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain_openai import OpenAI,ChatOpenAI\n",
    "\n",
    "agent = create_pandas_dataframe_agent(ChatOpenAI(temperature=0,model=\"gpt-4\"), df, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The question seems to be asking for an identity, but given the context, it's not clear what identity is being asked for. It could be asking for the identity of the person answering the question, or it could be asking for the identity of a person in the dataframe. However, since this is a programming context and I am an AI model, I will assume it's a misdirected question.\n",
      "Final Answer: I am an AI model designed to assist with Python programming tasks.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"who are you\"\n",
    "response = agent(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders.csv_loader import UnstructuredCSVLoader,CSVLoader\n",
    "# df = pd.read_csv('biostats.csv')\n",
    "loader = CSVLoader(file_path=\"biostats.csv\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### ingest part ##########\n",
    "\n",
    "###1:csv file along with metadata.\n",
    "  #2:getting data frame of csv data.\n",
    "#3:use openai gpt for making all the descriptions about csv file.it will be in a string format.\n",
    "#4:use embeddings everything and store those details along with metadata in vector store.\n",
    "########### qa part  ###########\n",
    "\n",
    "#1:question converted to embeddings search for similiarity .\n",
    "#2:retrieved context .\n",
    "#3:find document id and fetch data from mongodb.\n",
    "#4:make data frame .\n",
    "#5:using data frame agent create answers and return the answer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # data = request.get_json()\n",
    "    # doc_id = data.get('DocumentID')\n",
    "    # # base64_doc = data.get('FileData')\n",
    "    # filename = data.get('FileName')\n",
    "    # space_name = data.get('SpaceName')\n",
    "    # space_id = data.get('SpaceID')\n",
    "    # uploader_name = data.get('UploadedBy')\n",
    "    # updated_time = data.get('UpdatedDate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m metadata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocumentid\u001b[39m\u001b[38;5;124m'\u001b[39m: doc_id,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m: filename,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacename\u001b[39m\u001b[38;5;124m'\u001b[39m: [space_name],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspaceid\u001b[39m\u001b[38;5;124m'\u001b[39m:space_id,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muploadername\u001b[39m\u001b[38;5;124m'\u001b[39m: uploader_name,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdateddate\u001b[39m\u001b[38;5;124m\"\u001b[39m : updated_time\n\u001b[0;32m      8\u001b[0m     }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc_id' is not defined"
     ]
    }
   ],
   "source": [
    "metadata = {\n",
    "    'documentid': doc_id,\n",
    "    'filename': filename,\n",
    "    'spacename': [space_name],\n",
    "    'spaceid':space_id,\n",
    "    'uploadername': uploader_name,\n",
    "    \"updateddate\" : updated_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '.' in filename and filename.rsplit('.', 1)[1].lower() in \"csv\":\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_content = \"\\n\".join([doc.page_content for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"merged_biostats.txt\", \"w\") as f:\n",
    "#     f.write(merged_content)\n",
    "\n",
    "# with open(\"merged_biostats.txt\", \"r\") as f:\n",
    "#     text_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
